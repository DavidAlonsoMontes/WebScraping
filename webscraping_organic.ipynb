{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScrapping\n",
    "Vamos a realizar un webscrapping de un directorio de empresas\n",
    "\n",
    "Dado que la estructura de navegación del directorio es similar a un arbol, iremos analizando y extrayendo los datos\n",
    "\n",
    "a medida que necesitemos desde la raiz del arbol hasta las hojas. Las hojas sería los datos de cada compañia\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestra tabla inicial pretendemos sacar una seríe de datos, que son los productos, compradores y vendedores\n",
    "```htm\n",
    "<div id=\"categorytable\">\n",
    "    <table>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>\n",
    "                <ul class=\"ul_home_products\">\n",
    "                    <li><a href=\"abanico/\">ABANICO</a>&nbsp;<span class=\"home_sellers\"><a href=\"abanico/?groupService=46\">1</a></span>&nbsp;<span class=\"home_buyers\"><a href=\"abanico/?groupService=45\">0</a></span></li>\n",
    "                    <li><a href=\"albaricoques/\">ALBARICOQUES</a>&nbsp;<span class=\"home_sellers\"><a href=\"albaricoques/?groupService=46\">29</a></span>&nbsp;<span class=\"home_buyers\"><a href=\"albaricoques/?groupService=45\">7</a></span></li>\n",
    "                </ul>\n",
    "                </td>\n",
    "                <td>\n",
    "                <ul class=\"ul_home_products\">\n",
    "                    <li><a href=\"datiles/\">DATILES</a>&nbsp;<span class=\"home_sellers\"><a href=\"datiles/?groupService=46\">18</a></span>&nbsp;<span class=\"home_buyers\"><a href=\"datiles/?groupService=45\">4</a></span></li>\n",
    "                    <li><a href=\"endrinas/\">ENDRINAS</a>&nbsp;<span class=\"home_sellers\"><a href=\"endrinas/?groupService=46\">0</a></span>&nbsp;<span class=\"home_buyers\"><a href=\"endrinas/?groupService=45\">0</a></span></li>\n",
    "                </ul>\n",
    "                </td>\n",
    "                <td>\n",
    "                <ul class=\"ul_home_products\">\n",
    "                    <li><a href=\"limones/\">LIMONES</a>&nbsp;<span class=\"home_sellers\"><a href=\"limones/?groupService=46\">38</a></span>&nbsp;<span class=\"home_buyers\"><a href=\"limones/?groupService=45\">13</a></span></li>\n",
    "                    <li><a href=\"litchis/\">LITCHIS</a>&nbsp;<span class=\"home_sellers\"><a href=\"litchis/?groupService=46\">4</a></span>&nbsp;<span class=\"home_buyers\"><a href=\"litchis/?groupService=45\">1</a></span></li>\n",
    "                </ul>\n",
    "                </td>\n",
    "                <td>\n",
    "                <ul class=\"ul_home_products\">\n",
    "                    <li><a href=\"naranjas/\">NARANJAS</a>&nbsp;<span class=\"home_sellers\"><a href=\"naranjas/?groupService=46\">46</a></span>&nbsp;<span class=\"home_buyers\"><a href=\"naranjas/?groupService=45\">13</a></span></li>\n",
    "                    <li><a href=\"nectarinas/\">NECTARINAS</a>&nbsp;<span class=\"home_sellers\"><a href=\"nectarinas/?groupService=46\">16</a></span>&nbsp;<span class=\"home_buyers\"><a href=\"nectarinas/?groupService=45\">2</a></span></li>\n",
    "                </ul>\n",
    "                </td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El codigo html donde debemos extraer los datos para completar nuestra tabla de inicio es: \n",
    "```html\n",
    "<li>\n",
    "    <a href=\"abanico/\">ABANICO</a>\n",
    "    <span class=\"home_sellers\">\n",
    "        <a href=\"abanico/?groupService=46\">1</a>\n",
    "    </span>\n",
    "    <span class=\"home_buyers\">\n",
    "        <a href=\"abanico/?groupService=45\">0</a>\n",
    "    </span>\n",
    "</li>\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL de la página\n",
    "url = \"https://www.organic-bio.com/es/directorio/frutos/\"\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Encontrar la tabla de categorías\n",
    "category_table = soup.find(\"div\", id=\"categorytable\")\n",
    "\n",
    "# Lista para almacenar los datos\n",
    "products = []\n",
    "\n",
    "# Base URL para completar enlaces\n",
    "base_url = \"https://www.organic-bio.com/es/directorio/frutos/\"\n",
    "\n",
    "# Extraer los productos\n",
    "if category_table:\n",
    "    for li in category_table.find_all(\"li\"):\n",
    "        product_name = li.find(\"a\").text.strip()\n",
    "\n",
    "        # Buscar enlaces de vendedores y compradores de forma segura\n",
    "        seller_span = li.find(\"span\", class_=\"home_sellers\")\n",
    "        buyer_span = li.find(\"span\", class_=\"home_buyers\")\n",
    "\n",
    "        seller_link = seller_span.find(\"a\")[\"href\"] if seller_span and seller_span.find(\"a\") else None\n",
    "        buyer_link = buyer_span.find(\"a\")[\"href\"] if buyer_span and buyer_span.find(\"a\") else None\n",
    "\n",
    "        #Como el atributo href solo tiene la direccion relativa, debemos añadir url base para completar la dirección url completa\n",
    "        full_seller_url = base_url + seller_link if seller_link else \"No disponible\"\n",
    "        full_buyer_url = base_url + buyer_link if buyer_link else \"No disponible\"\n",
    "\n",
    "        #Vamos añadiendo en la lista de productos un diccionario con los tres campos que necesitamos\n",
    "        # Producto, Vendedores y Compradores\n",
    "        products.append({\n",
    "            \"Producto\": product_name,\n",
    "            \"Vendedores\": full_seller_url,\n",
    "            \"Compradores\": full_buyer_url\n",
    "        })\n",
    "print(f'Lista de productos: {products}')\n",
    "# Guardar en Excel\n",
    "df = pd.DataFrame(products)\n",
    "#print(df)\n",
    "df.to_excel(\"productos.xlsx\", index=False, sheet_name=\"Productos\")\n",
    "\n",
    "print(\"Datos extraídos y guardados en productos2.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de una tabla que hemos creado en la que tenemos productos y los enlaces a la categoria de vendedores y compradores\n",
    "de ese determinado producto, recorreremos todos estos enlaces para ir obteniendo el directorio completo de los datos de las empresas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Leer la tabla de productos\n",
    "productos_df = pd.read_excel(\"productos.xlsx\")\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://www.organic-bio.com/es/\"\n",
    "\n",
    "def obtener_paginas(url):\n",
    "    \"\"\"Obtiene todas las páginas de un listado paginado\"\"\"\n",
    "    paginas = [url]\n",
    "    print(f'La pagina donde se tienen que obtener los enlaces a las diversas paginas es:\\n{url}')\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    pages_div = soup.find(\"div\", id=\"pages\")\n",
    "    if pages_div and pages_div.find_all(\"a\"):\n",
    "        # Vamos a guardar las paginas en un conjunto para evitar los duplicados,\n",
    "        # ya que el enlace a siguente es un enlace duplicado en el <div id=\"pages\">\n",
    "        paginas=set()\n",
    "        url_base=url.split(\"?\")[0]\n",
    "        for a in pages_div.find_all(\"a\"):\n",
    "            page_link = a[\"href\"]\n",
    "            if page_link not in paginas:\n",
    "                paginas.add(url_base + page_link)\n",
    "    print(f'Paginas: {paginas}')\n",
    "    return paginas\n",
    "\n",
    "\n",
    "\n",
    "def obtener_empresas(url):\n",
    "    \"\"\"Obtiene los enlaces a las empresas desde una página donde esta el listado de estas\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error al acceder a {url}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    empresas = []\n",
    "    \n",
    "    # Buscar la tabla con id=\"firms\"\n",
    "    tabla = soup.find('table', {'id': 'firms'})\n",
    "    if not tabla:\n",
    "        print(\"No se encontró la tabla de empresas en la página.\")\n",
    "        return []\n",
    "    \n",
    "    # Recorrer las filas de la tabla\n",
    "    for fila in tabla.find_all('tr')[1:]:  # Saltamos la primera fila que es la cabecera\n",
    "        celda = fila.find('td')  # Primera celda de la fila\n",
    "        if celda:\n",
    "            enlace = celda.find('a')\n",
    "            if enlace and 'href' in enlace.attrs:\n",
    "                empresas.append(enlace['href'])\n",
    "    print(f'Empresas: {empresas}')\n",
    "    return empresas\n",
    "\n",
    "\n",
    "def extraer_datos_empresa(url):\n",
    "    \n",
    "    '''Recibe un enlace donde se encuentra la tabla donde están los datos de la empresa\n",
    "        y a partir de este enlace y la tabla obtiene todos los datos que se almacenarán en un diccionario\n",
    "        con las keys como los campos de la tabla y los values como los valores de los campos'''\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error al acceder a {url}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tabla = soup.find('table', {'id': 'company'})\n",
    "    \n",
    "    if not tabla:\n",
    "        print(\"No se encontró la tabla de la empresa en la página.\")\n",
    "        return None\n",
    "    \n",
    "    datos_empresa = {}\n",
    "    \n",
    "    for fila in tabla.find_all('tr'):\n",
    "        columnas = fila.find_all(['th', 'td'])\n",
    "        if len(columnas) == 2:\n",
    "            campo = columnas[0].text.strip()\n",
    "            valor = columnas[1].text.strip()\n",
    "            datos_empresa[campo] = valor\n",
    "    \n",
    "    return datos_empresa\n",
    "\n",
    "# Lista para almacenar los datos finales\n",
    "empresas_datos = []\n",
    "\n",
    "# Recorrer la tabla de productos\n",
    "for index, row in productos_df.iterrows():\n",
    "    #print(f'Index: {index} y Row: {row[\"Vendedores\"]}')\n",
    "    producto = row[\"Producto\"]\n",
    "    for categoria, url in [(\"Vendedor\", row[\"Vendedores\"]), (\"Comprador\", row[\"Compradores\"])]:\n",
    "        print(f'Producto: {producto} \\nCategoria: {categoria}\\nUrl: {url}')\n",
    "        paginas=obtener_paginas(url)\n",
    "        \n",
    "        if url != \"No disponible\":\n",
    "            #Vamos a recorrer el listado completo buscando en todas las paginas\n",
    "            # de el producto y la categoria en cuestion\n",
    "            for pagina in paginas:\n",
    "                empresas = obtener_empresas(pagina)\n",
    "                for empresa_url in empresas:\n",
    "                    datos_empresa = extraer_datos_empresa(empresa_url)\n",
    "                    # Guardamos los productos y la categoría a la que pertenece la empresa en los datos de la empresa\n",
    "                    datos_empresa[\"Producto\"] = producto\n",
    "                    datos_empresa[\"Categoría\"] = categoria\n",
    "                    #Añadimos la empresa al listado de empresas\n",
    "                    empresas_datos.append(datos_empresa)\n",
    "                    time.sleep(0.1)  # Para evitar bloqueos\n",
    "\n",
    "# Guardar en un nuevo Excel\n",
    "df_final = pd.DataFrame(empresas_datos)\n",
    "#df_final.to_excel(\"empresas.xlsx\", index=False, sheet_name=\"Empresas\")\n",
    "\n",
    "print(\"Proceso completado. Datos guardados en empresas2.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
